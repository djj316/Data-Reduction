
# æ•°æ®å½’çº¦æŠ€æœ¯å®éªŒæŠ¥å‘Šï¼šåŸºäº Wine Quality æ•°æ®é›†çš„ PCA ä¸ LDA åˆ†æ

**GitHub ä»“åº“**ï¼š[https://github.com/djj316/Data-Reduction](https://github.com/djj316/Data-Reduction)  
**æœ€åæ›´æ–°**ï¼š2025å¹´4æœˆ13æ—¥  

---

## ç›®å½•
1. [å®éªŒç›®çš„](#1-å®éªŒç›®çš„)  
2. [æ•°æ®é›†](#2-æ•°æ®é›†)  
3. [æ–¹æ³•](#3-æ–¹æ³•)  
4. [å®éªŒç»“æœ](#4-å®éªŒç»“æœ)  
5. [ç»“æœåˆ†æä¸è®¨è®º](#5-ç»“æœåˆ†æä¸è®¨è®º)  
6. [ç»“è®º](#6-ç»“è®º)  
7. [é™„å½•](#7-é™„å½•)  

---

## 1. å®éªŒç›®çš„
æœ¬å®éªŒæ—¨åœ¨æ¢è®¨ä¸¤ç§ç»å…¸é™ç»´æŠ€æœ¯â€”â€”ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysis, PCAï¼‰ä¸çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear Discriminant Analysis, LDAï¼‰åœ¨è‘¡è„é…’è´¨é‡åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœï¼Œå…·ä½“ç›®æ ‡å¦‚ä¸‹ï¼š

- æ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•å¯¹åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼›
- å¯è§†åŒ–é«˜ç»´æ•°æ®åœ¨ä½ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒç‰¹å¾ï¼›
- åˆ†ææ–¹å·®ä¿ç•™ç‡ä¸ç»´åº¦å‹ç¼©ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼›
- è¯„ä¼°æ•°æ®è§„çº¦å¯¹åç»­å­¦ä¹ ä»»åŠ¡çš„ä½œç”¨ä¸å½±å“ã€‚

---

## 2. æ•°æ®é›†

### æ•°æ®æ¥æº
å®éªŒæ‰€ä½¿ç”¨çš„æ•°æ®é›†æ¥æºäº UCI æœºå™¨å­¦ä¹ ä»“åº“ï¼š  
ğŸ‘‰ [Wine Quality Dataset (ID:186)](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)

### ç‰¹å¾æè¿°

| ç‰¹å¾ç±»åˆ« | æ•°é‡ | ç¤ºä¾‹ç‰¹å¾                 |
|----------|------|--------------------------|
| ç†åŒ–æŒ‡æ ‡ | 11   | é…¸åº¦ã€pHå€¼ã€é…’ç²¾æµ“åº¦ç­‰   |
| ç›®æ ‡å˜é‡ | 1    | è´¨é‡è¯„åˆ†ï¼ˆèŒƒå›´ 3~9ï¼‰     |

### æ•°æ®é¢„å¤„ç†
ä¸ºç®€åŒ–åˆ†ç±»ä»»åŠ¡ï¼Œå°†åŸå§‹çš„è‘¡è„é…’è´¨é‡è¯„åˆ†ç¦»æ•£åŒ–ä¸ºä¸‰ä¸ªç­‰çº§ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰ï¼š

```python
# å°†è¯„åˆ†æŒ‰é˜ˆå€¼è¿›è¡Œåˆ†ç®±ï¼Œå¾—åˆ°ä¸‰ç±»æ ‡ç­¾
y = np.digitize(y, bins=[3, 6], right=True) - 1
```

æ­¤å¤–ï¼Œæ•°æ®é›†æŒ‰ç…§ 7:3 çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å¯¹ç‰¹å¾è¿›è¡Œäº†æ ‡å‡†åŒ–å¤„ç†ä»¥é€‚åº” PCA å¤„ç†è¦æ±‚ã€‚

---

## 3. æ–¹æ³•

### æŠ€æœ¯æµç¨‹æ¦‚è¿°

```mermaid
graph LR
A[åŸå§‹æ•°æ®] --> B[æ ‡å‡†åŒ–]
B --> C[PCAé™ç»´]
B --> D[LDAé™ç»´]
C --> E[KNNåˆ†ç±»å™¨ - PCA]
D --> F[KNNåˆ†ç±»å™¨ - LDA]
B --> G[KNNåˆ†ç±»å™¨ - åŸå§‹æ•°æ®]
```

æœ¬å®éªŒé‡‡ç”¨ KNNï¼ˆK-è¿‘é‚»ï¼‰ä½œä¸ºç»Ÿä¸€çš„åˆ†ç±»å™¨ï¼Œå¯¹åŸå§‹ç‰¹å¾ã€PCA é™ç»´åç‰¹å¾ä»¥åŠ LDA é™ç»´åç‰¹å¾åˆ†åˆ«è¿›è¡Œåˆ†ç±»è¯„ä¼°ã€‚PCA ä¸ºæ— ç›‘ç£é™ç»´æ–¹æ³•ï¼Œä¸»è¦åŸºäºæ•°æ®æ–¹å·®ï¼›è€Œ LDA å±äºç›‘ç£å¼æ–¹æ³•ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç±»é—´è·ç¦»ä¸æœ€å°åŒ–ç±»å†…è·ç¦»ã€‚

---

## 4. å®éªŒç»“æœ

### 4.1 åˆ†ç±»å‡†ç¡®ç‡æ¯”è¾ƒ

| æ–¹æ³•         | æµ‹è¯•å‡†ç¡®ç‡ | é™ç»´åç»´åº¦ |
|--------------|------------|------------|
| åŸå§‹ç‰¹å¾é›†   | 0.80       | 11         |
| PCA é™ç»´      | 0.84       | 9          |
| LDA é™ç»´      | 0.82       | 2          |

### 4.2 å¯è§†åŒ–ç»“æœ

#### PCA ç´¯è®¡æ–¹å·®è§£é‡Šç‡  
![PCA Cumulative Variance](./PCAç´¯è®¡è§£é‡Šæ–¹å·®å›¾%20.png)

#### ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®åº¦åˆ†æ  
![PCA Variance](./ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®åº¦.png)

#### LDA ä¸ PCA äºŒç»´æŠ•å½±å¯¹æ¯”  
![Projection Comparison](./LDAä¸PCAæŠ•å½±å¯è§†åŒ–%20.png)

#### åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”å›¾  
![Accuracy Comparison](./æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”%20.png)

---

## 5. ç»“æœåˆ†æä¸è®¨è®º

### PCA æ–¹æ³•åˆ†æ
- å‰ä¸¤ä¸ªä¸»æˆåˆ†å…±è§£é‡Šçº¦ 50% çš„æ–¹å·®ï¼Œè¯´æ˜æ•°æ®åœ¨å‰ä¸¤ä¸ªç»´åº¦ä¸Šä»å­˜åœ¨å¤§é‡ä¿¡æ¯æŸå¤±ï¼›
- åœ¨ä¿ç•™ 95% æ–¹å·®çš„å‰æä¸‹ï¼Œå¯å°†ç»´åº¦ä» 11 é™è‡³ 9ï¼Œé™ç»´æ•ˆæœæ˜¾è‘—ï¼›
- ç”±äº PCA ä¸ºæ— ç›‘ç£æ–¹æ³•ï¼Œå…¶ä½ç»´æŠ•å½±å¯èƒ½æœªèƒ½æœ‰æ•ˆçªå‡ºç±»åˆ«é—´å·®å¼‚ï¼Œå› æ­¤åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç•¥é€Šäº LDAï¼›
- é€‚åˆç”¨äºæ¢ç´¢æ€§æ•°æ®åˆ†æä¸å¯è§†åŒ–ã€‚

### LDA æ–¹æ³•åˆ†æ
- å°½ç®¡è¢«çº¦æŸè‡³äºŒç»´ç©ºé—´ï¼ŒLDA ä»èƒ½ç»´æŒè¾ƒé«˜çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå±•ç¤ºå‡ºè‰¯å¥½çš„ç±»åˆ«åˆ¤åˆ«èƒ½åŠ›ï¼›
- LDA é€šè¿‡ç›‘ç£å­¦ä¹ æ˜¾å¼æœ€å¤§åŒ–ç±»é—´è·ç¦»ï¼Œæå‡äº†ä½ç»´ç©ºé—´çš„å¯åˆ†æ€§ï¼›
- ç†è®ºä¸Šï¼ŒLDA çš„æŠ•å½±ç»´åº¦ä¸è¶…è¿‡ç±»åˆ«æ•°å‡ä¸€ï¼ˆC-1ï¼‰ï¼Œæœ¬å®éªŒä¸­ä¸º 2 ç»´ï¼Œé™åˆ¶äº†é™ç»´çµæ´»æ€§ï¼›
- æ›´é€‚ç”¨äºæœ‰ç›‘ç£çš„é™ç»´ä¸å¯è§†åŒ–åœºæ™¯ã€‚

---

## 6. ç»“è®º

ç»“åˆå®éªŒç»“æœï¼Œå¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

1. åœ¨ä¿ç•™ 95% æ€»æ–¹å·®çš„å‰æä¸‹ï¼ŒPCA å¯æœ‰æ•ˆå°†åŸå§‹ç‰¹å¾ç»´åº¦ä» 11 é™è‡³ 9ï¼Œç»´åº¦å‹ç¼©ç‡ä¸º 18.2%ï¼›
2. LDA è™½ä»…ä¿ç•™ä¸¤ä¸ªç»´åº¦ï¼Œä½†å…¶ç›‘ç£æ€§è´¨ä½¿å¾—åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šä¼˜äºåŸå§‹ç‰¹å¾ï¼Œä¸”ä¸ PCA è¡¨ç°æ¥è¿‘ï¼›
3. **å®ç”¨å»ºè®®**ï¼š
   - è¿›è¡Œç‰¹å¾æ¢ç´¢æˆ–å¯è§†åŒ–æ—¶ï¼Œæ¨èä¼˜å…ˆä½¿ç”¨ PCAï¼›
   - è‹¥ç›®æ ‡ä¸ºæå‡åˆ†ç±»æ€§èƒ½ï¼Œä¸”å…·æœ‰å¯é æ ‡ç­¾ä¿¡æ¯ï¼Œåˆ™å»ºè®®ä½¿ç”¨ LDAï¼›
   - åœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­å¯ç»“åˆä¸¤è€…è¿›è¡Œç»¼åˆè¯„ä¼°ä¸é€‰æ‹©ã€‚

---

## 7. é™„å½•

### å®éªŒç¯å¢ƒ

```bash
Python ç‰ˆæœ¬ï¼š3.8+  
ä¾èµ–åº“ï¼š  
- numpy >= 1.21  
- scikit-learn >= 1.0  
- matplotlib >= 3.5  
```

### å®Œæ•´æºä»£ç 
è¯¦è§æœ¬æ–‡æœ«å°¾ä»£ç å—ï¼Œæˆ–è®¿é—® GitHub ä»“åº“è·å–ï¼š[Data-Reduction](https://github.com/djj316/Data-Reduction)

<details>
<summary>ğŸ“„ ç‚¹å‡»å±•å¼€å®Œæ•´æºä»£ç </summary>

```python
from ucimlrepo import fetch_ucirepo
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from matplotlib.colors import Normalize
import matplotlib

# ä¸­æ–‡è®¾ç½®
norm = Normalize(vmin=0, vmax=2)
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# 1. è·å– Wine Quality æ•°æ®é›†
wine_quality = fetch_ucirepo(id=186)
X = np.array(wine_quality.data.features)
y = np.array(wine_quality.data.targets).flatten()

# 2. å°†è´¨é‡è¯„åˆ†è½¬æ¢ä¸ºä¸‰ç±»æ ‡ç­¾
y = np.where(y < 4, 0, np.where(y < 7, 1, 2))

# 3. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 4. æ ‡å‡†åŒ–ç”¨äº PCA
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. å¯»æ‰¾ä¿ç•™95%æ–¹å·®çš„PCAç»´åº¦æ•°
pca_full = PCA()
pca_full.fit(X_train_scaled)
cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1
print(f"ä¿ç•™ 95% æ–¹å·®æ‰€éœ€ PCA ç»´åº¦æ•°: {n_components_95}")

# 6. ä½¿ç”¨ PCA é™ç»´
pca = PCA(n_components=n_components_95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# 7. ä½¿ç”¨ LDA é™ç»´
lda = LinearDiscriminantAnalysis(n_components=2)
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)

# 8. ä½¿ç”¨ KNN è¿›è¡Œè®­ç»ƒä¸é¢„æµ‹
knn_lda = KNeighborsClassifier(n_neighbors=3).fit(X_train_lda, y_train)
knn_pca = KNeighborsClassifier(n_neighbors=3).fit(X_train_pca, y_train)
knn_raw = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)

# 9. è¯„ä¼°å‡†ç¡®ç‡
accuracy_lda = accuracy_score(y_test, knn_lda.predict(X_test_lda))
accuracy_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))
accuracy_raw = accuracy_score(y_test, knn_raw.predict(X_test))

print(f"Accuracy with LDA: {accuracy_lda:.2f}")
print(f"Accuracy with PCA (è‡ªåŠ¨ç»´åº¦={n_components_95}): {accuracy_pca:.2f}")
print(f"Accuracy without LDA or PCA: {accuracy_raw:.2f}")

# 10. å¯è§†åŒ– LDA å’Œ PCA é™ç»´ç»“æœ
plt.figure(figsize=(14, 6))

# LDA å¯è§†åŒ–
plt.subplot(1, 2, 1)
scatter_lda = plt.scatter(X_test_lda[:, 0], X_test_lda[:, 1],
                        c=y_test, cmap='viridis', edgecolor='k', s=80)
plt.title("LDA: 2D Projection")
plt.xlabel("LDA Component 1")
plt.ylabel("LDA Component 2")
cbar_lda = plt.colorbar(scatter_lda, ticks=[0, 1, 2])
cbar_lda.ax.set_yticklabels(['ä½è´¨é‡', 'ä¸­è´¨é‡', 'é«˜è´¨é‡'])

# PCA å¯è§†åŒ–
plt.subplot(1, 2, 2)
scatter_pca = plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1],
                        c=y_test, cmap='viridis', edgecolor='k', s=80)
plt.title("PCA: First 2 Components")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
cbar_pca = plt.colorbar(scatter_pca, ticks=[0, 1, 2])
cbar_pca.ax.set_yticklabels(['ä½è´¨é‡', 'ä¸­è´¨é‡', 'é«˜è´¨é‡'])

plt.tight_layout()
plt.savefig("LDAä¸PCAæŠ•å½±å¯è§†åŒ–.png", dpi=300)
plt.show()

# 11. PCA ç´¯è®¡è§£é‡Šæ–¹å·®å›¾
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% æ–¹å·®')
plt.xlabel("PCA ç»´åº¦æ•°")
plt.ylabel("ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”")
plt.title("PCA ç´¯è®¡è§£é‡Šæ–¹å·®å›¾")
plt.grid(True)
plt.legend()
plt.savefig("PCAç´¯è®¡è§£é‡Šæ–¹å·®å›¾.png", dpi=300)
plt.show()

# 12. æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
plt.figure(figsize=(8, 6))
models = ['LDA', 'PCA', 'åŸå§‹æ•°æ®']
accuracies = [accuracy_lda, accuracy_pca, accuracy_raw]
colors = ['skyblue', 'lightgreen', 'salmon']

bars = plt.bar(models, accuracies, color=colors, edgecolor='black')

for bar in bars:
   yval = bar.get_height()
   plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.015,
            f'{yval:.2f}', ha='center', va='bottom', fontsize=12)

plt.ylim(0, 1.05)
plt.title('ä¸åŒé™ç»´æ–¹å¼å¯¹KNNå‡†ç¡®ç‡çš„å½±å“', fontsize=15)
plt.ylabel('å‡†ç¡®ç‡', fontsize=13)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.savefig("æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”.png", dpi=300)
plt.show()

# 14. ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®åº¦åˆ†æ
plt.figure(figsize=(10, 6))
explained_variance = pca.explained_variance_ratio_
cumulative = np.cumsum(explained_variance)

plt.bar(range(1, len(explained_variance)+1), 
      explained_variance, 
      alpha=0.6,
      color='g',
      label='å•ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®')

plt.step(range(1, len(cumulative)+1), 
         cumulative, 
         where='mid',
         label='ç´¯è®¡è§£é‡Šæ–¹å·®',
         color='r')

plt.axhline(y=0.95, color='b', linestyle='--', label='95%æ–¹å·®é˜ˆå€¼')
plt.xlabel("ä¸»æˆåˆ†æ•°é‡")
plt.ylabel("è§£é‡Šæ–¹å·®æ¯”ä¾‹")
plt.title("å„ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®åº¦åˆ†æ", fontsize=14)
plt.legend(loc='best')
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.savefig("ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®åº¦.png", dpi=300)
plt.show()
```

</details>

---

**æŠ¥å‘Šä½œè€…**ï¼š  
```
zyh
```

